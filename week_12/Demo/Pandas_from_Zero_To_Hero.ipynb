{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "da797e1a",
      "metadata": {
        "id": "da797e1a"
      },
      "source": [
        "# Intro to pandas\n",
        "Pandas is a powerful and flexible Python library for data manipulation and analysis. It provides data structures like Series and DataFrame, which are essential for handling structured data efficiently.\n",
        "\n",
        "Resources: https://pandas.pydata.org/pandas-docs/version/1.4/pandas.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b850852d",
      "metadata": {
        "id": "b850852d"
      },
      "source": [
        "Pandas is widely used in data science, machine learning, and data analysis workflows due to its ease of use and integration with other libraries like NumPy, Matplotlib, and Scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45349881",
      "metadata": {
        "id": "45349881"
      },
      "source": [
        "# Installing and Importing pandas\n",
        "To use pandas, you need to install it first. You can install pandas using pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a535b4a",
      "metadata": {
        "id": "6a535b4a"
      },
      "outputs": [],
      "source": [
        "# Installing pandas\n",
        "# Run this command in your terminal or Jupyter Notebook\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9441dce5",
      "metadata": {
        "id": "9441dce5"
      },
      "outputs": [],
      "source": [
        "# Importing pandas\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc33b817",
      "metadata": {
        "id": "dc33b817"
      },
      "source": [
        "# Creating DataFrames\n",
        "DataFrames are one of the core data structures in pandas. You can create them from various data sources like dictionaries, lists, and NumPy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11fff2ce",
      "metadata": {
        "id": "11fff2ce"
      },
      "outputs": [],
      "source": [
        "# Creating a DataFrame from a dictionary\n",
        "data = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35], 'City': ['New York', 'Nan', 'Chicago']}\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "ZfjN3dtPvklO"
      },
      "id": "ZfjN3dtPvklO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f6d161",
      "metadata": {
        "id": "a8f6d161"
      },
      "outputs": [],
      "source": [
        "# Creating a DataFrame from a list of lists\n",
        "data = [['Alice', 25, 'New York'], ['Bob', 30, 'Los Angeles'], ['Charlie', 35, 'Chicago']]\n",
        "df = pd.DataFrame(data, columns=['Name', 'Age', 'State'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f1119eb",
      "metadata": {
        "id": "8f1119eb"
      },
      "outputs": [],
      "source": [
        "# Creating a DataFrame from a NumPy array\n",
        "import numpy as np\n",
        "data = np.array([['Alice', 25, 'New York'], ['Bob', 30, 'Los Angeles'], ['Charlie', 35, 'Chicago']])\n",
        "df = pd.DataFrame(data, columns=['Name', 'Age', 'City'])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a5e11c6",
      "metadata": {
        "id": "4a5e11c6"
      },
      "source": [
        "# Reading Data from Files\n",
        "Pandas makes it easy to read data from various file formats like CSV, Excel, and JSON."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# new_path=\"https://raw.githubusercontent.com/DAPLearning2025/materials/refs/heads/main/Resources%20%26%20Data/oecd_bli_2015.csv\"\n",
        "# df_csv= pd.read_csv(new_path)\n",
        "# df_csv.head(10)"
      ],
      "metadata": {
        "id": "vHUrlarEcdk0"
      },
      "id": "vHUrlarEcdk0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be871185",
      "metadata": {
        "id": "be871185"
      },
      "outputs": [],
      "source": [
        "# Reading data from a CSV file\n",
        "path =\"https://raw.githubusercontent.com/DAPLearning2025/materials/refs/heads/main/Resources%20%26%20Data/oecd_bli_2015.csv\"\n",
        "df_csv= pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv.head(3)"
      ],
      "metadata": {
        "id": "ZfAQUBKZcwoU"
      },
      "id": "ZfAQUBKZcwoU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02caab42",
      "metadata": {
        "id": "02caab42"
      },
      "outputs": [],
      "source": [
        "# Reading data from an Excel file\n",
        "# # !pip install openpyxl\n",
        "file_url = \"https://github.com/DAPLearning2025/materials/raw/main/Resources%20%26%20Data/sample-2-sheets.xlsx\"\n",
        "df_xlsx = pd.read_excel(file_url, sheet_name=1, engine='openpyxl')  # sheet_name=0 reads the first sheet (index starts at 0)\n",
        "df_xlsx.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8367d871",
      "metadata": {
        "id": "8367d871"
      },
      "outputs": [],
      "source": [
        "# Reading data from a JSON file\n",
        "path = \"https://raw.githubusercontent.com/DAPLearning2025/materials/refs/heads/main/Resources%20%26%20Data/product_names.json\"\n",
        "df_json = pd.read_json(path)\n",
        "df_json.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0d3fa8d",
      "metadata": {
        "id": "e0d3fa8d"
      },
      "source": [
        "# DataFrame Inspection\n",
        "Once you have a DataFrame, you can inspect its structure and contents using various methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "349cb5fc",
      "metadata": {
        "id": "349cb5fc"
      },
      "outputs": [],
      "source": [
        "# Reading data from a CSV file\n",
        "path =\"https://raw.githubusercontent.com/DAPLearning2025/materials/refs/heads/main/Resources%20%26%20Data/oecd_bli_2015.csv\"\n",
        "df_csv = pd.read_csv(path)\n",
        "\n",
        "# Columns\n",
        "print(\"Wanted to see the dataframe: Columns\")\n",
        "print(df.columns)\n",
        "\n",
        "#how many rows\n",
        "print(\"\\nWanted to see the dataframe: #rows\")\n",
        "print(df_csv.shape) # I can also use df.shape\n",
        "\n",
        "#Get to know the type for each field\n",
        "print(\"\\nWanted to see the dataframe: Files data type\")\n",
        "print(df_csv.dtypes)\n",
        "\n",
        "# Getting a summary of the DataFrame\n",
        "print(\"\\nWanted to see the dataframe: Get info\")\n",
        "print(df_csv.info())\n",
        "\n",
        "# Getting descriptive statistics\n",
        "print(\"\\nWanted to see the dataframe: Get description\")\n",
        "print(df_csv.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Looking at Columns, Rows, and Cells"
      ],
      "metadata": {
        "id": "B-28DCURB9TA"
      },
      "id": "B-28DCURB9TA"
    },
    {
      "cell_type": "code",
      "source": [
        "# get the country column and save it to its own variable\n",
        "country_df=df_csv['Country']\n",
        "country_df # You can use head, tail or specify how many rows would like to see"
      ],
      "metadata": {
        "id": "dZP2xNLkSjd3"
      },
      "id": "dZP2xNLkSjd3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv['Country'].unique()"
      ],
      "metadata": {
        "id": "o0fsu7FKh454"
      },
      "id": "o0fsu7FKh454",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#looking at Country, Inequality, and Value\n",
        "subset = df_csv[[\"Country\",\"Inequality\",\"Value\"]]\n",
        "subset.head(3)"
      ],
      "metadata": {
        "id": "PnVjGdZMS6xs"
      },
      "id": "PnVjGdZMS6xs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Subset rows by index label: Loc\n",
        "###########################################\n",
        "# Subset method                         ### Description\n",
        "# loc                                       Subset based on index label (row name)\n",
        "# iloc                                      Subset based on row index (row number)\n",
        "###############\n",
        "# LOC[ Specify rows, specify columns] --> LOC[Specify left] --> Call all the columns for that specific rows\n",
        "\n",
        "\n",
        "#get the first row: Python count from 0\n",
        "# print(df_csv.loc[0]) # ? first row in dataframe\n",
        "# print(\"----\")\n",
        "# Get the 90th row\n",
        "df_csv.loc[89]\n",
        "\n",
        "#Get the last row\n",
        "print(\"----\")\n",
        "print(df_csv.loc[len(df_csv)-1]) # Think about another way? if you would say let's -1, be careful with that?\n",
        "# #another simple way:\n",
        "# last_row_index=df.shape[0]-1\n",
        "# print(\"----\")\n",
        "# print(df_csv.loc[last_row_index])"
      ],
      "metadata": {
        "id": "W5libMZ7T8H6"
      },
      "id": "W5libMZ7T8H6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_rows = df_csv.shape[0]\n",
        "df_csv.loc[total_rows-1]"
      ],
      "metadata": {
        "id": "rj7-FITpn_yT"
      },
      "id": "rj7-FITpn_yT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst=[23,435,65,67]\n",
        "lst[-1]"
      ],
      "metadata": {
        "id": "qc00hXwgnkNQ"
      },
      "id": "qc00hXwgnkNQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv.loc[[0,100,110]]"
      ],
      "metadata": {
        "id": "W2SOsBwoocWh"
      },
      "id": "W2SOsBwoocWh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#subset multiple rows\n",
        "# Inequality\tUnit Code\tUnit\n",
        "subset=df_csv[[\"Inequality\",\"Unit Code\",\"Unit\"]]\n",
        "subset.loc[3000::2]"
      ],
      "metadata": {
        "id": "u0GhMdLIVJZz"
      },
      "id": "u0GhMdLIVJZz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Subset rows using iloc\n",
        "###########################################\n",
        "df_csv.iloc[1] # which row am I geeting here?\n"
      ],
      "metadata": {
        "id": "8iJyGlvuW1rn"
      },
      "id": "8iJyGlvuW1rn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the last row\n",
        "df_csv.iloc[-1]"
      ],
      "metadata": {
        "id": "D5EuI8rmXX-Y"
      },
      "id": "D5EuI8rmXX-Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset.iloc[[0,99,999]]"
      ],
      "metadata": {
        "id": "3WtoxWIAjy_a"
      },
      "id": "3WtoxWIAjy_a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loc and iloc attributes can be used to obtain subsets of columns, rows, or both. The general syntax for iloc and loc use square brackets with a comma. The part to the left of the comma is the row values to subset. The part to the right of the comma is the column values to subset. That is, df.loc[rows],[columns]] or df.iloc[[rows],[columns]]"
      ],
      "metadata": {
        "id": "sKYdanAwkDat"
      },
      "id": "sKYdanAwkDat"
    },
    {
      "cell_type": "code",
      "source": [
        "#Subsetting columns\n",
        "subset=df_csv.loc[:,['Country','Value']]\n",
        "subset.head(20)"
      ],
      "metadata": {
        "id": "zYIiFCzTj48I"
      },
      "id": "zYIiFCzTj48I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv.columns"
      ],
      "metadata": {
        "id": "zMy3t_MqrL-d"
      },
      "id": "zMy3t_MqrL-d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_list = list(range(len(df_csv.columns)))\n",
        "second_list"
      ],
      "metadata": {
        "id": "f1rY_dqasCrt"
      },
      "id": "f1rY_dqasCrt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst1=[\"a\",\"b\",\"c\"]\n",
        "lst2=[1,2,3]\n",
        "dict(zip(lst1,lst2))"
      ],
      "metadata": {
        "id": "P7Fq1ksjsTgs"
      },
      "id": "P7Fq1ksjsTgs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the right index for each column\n",
        "print(df_csv.columns.get_loc(\"Country\")) # built in function\n",
        "print(df_csv.columns.get_loc(\"Value\"))\n",
        "# # I can use my code to determine that\n",
        "colum_index=dict(zip(df_csv.columns,range(len(df_csv.columns)))) # Get result in dictionnary format. Zip means combine tow things\n",
        "# first list = df_csv_columns, with second list\n",
        "print(colum_index)\n",
        "colum_index[\"Value\"]"
      ],
      "metadata": {
        "id": "dNfm5en1k0wz"
      },
      "id": "dNfm5en1k0wz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using iloc\n",
        "subset=df_csv.iloc[:,[1,14]]\n",
        "print(subset.head())\n",
        "# using iloc involved with -1\n",
        "subset=df_csv.iloc[:,[colum_index[\"Country\"],colum_index[\"Value\"], -1]]\n",
        "subset.head()"
      ],
      "metadata": {
        "id": "sO6jtxAVkxQE"
      },
      "id": "sO6jtxAVkxQE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the name of the columns directly\n",
        "subset=df_csv.loc[:,[\"Country\",\"Value\"]]\n",
        "subset.head()"
      ],
      "metadata": {
        "id": "WutkAuqDlvH_"
      },
      "id": "WutkAuqDlvH_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Subseting column using range\n",
        "subset=df_csv.iloc[:,range(2,5)] # range (2,5) will include 2 and exclude 5 --> 2,3,4\n",
        "subset.head()"
      ],
      "metadata": {
        "id": "_IoWt739mL2t"
      },
      "id": "_IoWt739mL2t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Subsetting rows and columns --> Tracking specific cell\n",
        "subset=df_csv.loc[43,[\"Country\"]]  # loc is smart enough to use the column names\n",
        "print(subset,\"\\n\")\n",
        "subset=df_csv.iloc[43,1] #iloc loves to go with indexes\n",
        "print(subset)"
      ],
      "metadata": {
        "id": "ANlXzDWEmSdM"
      },
      "id": "ANlXzDWEmSdM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Understanding & Manipulating the data"
      ],
      "metadata": {
        "id": "dOvQaq0hog7G"
      },
      "id": "dOvQaq0hog7G"
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting distinct values for various fields\n",
        "# We will target data in specific ways. Let's see the fields with no meaning context and collect its name\n",
        "Fields_to_drop =[]\n",
        "for field in df_csv.columns:\n",
        "  if len(df_csv[field].unique())<=1:\n",
        "    Fields_to_drop.append(field)\n",
        "    print(field, df_csv[field].unique())\n",
        "\n",
        "#df[\"MEASURE\"].unique()"
      ],
      "metadata": {
        "id": "ZPEYP4XUmSgH"
      },
      "id": "ZPEYP4XUmSgH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Fields_to_drop"
      ],
      "metadata": {
        "id": "JOpKxK3XwUiF"
      },
      "id": "JOpKxK3XwUiF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  A -- B (changing) -- C (changing) -- D    which columns provide solid context to data? C, B\n",
        "#  1 -- q             -- 1            -- 2     Columns A and D can be dropped\n",
        "#  1 -- C             -- 2            -- 2\n",
        "#  1 -- R              -- 3            -- 2"
      ],
      "metadata": {
        "id": "KrVAP2q8vDuJ"
      },
      "id": "KrVAP2q8vDuJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's drop the fileds we captured in previous cell\n",
        "print(\"The fields will be dropped are: \", Fields_to_drop)\n",
        "df_1=df_csv.drop(columns=Fields_to_drop)\n",
        "df_1.head()\n"
      ],
      "metadata": {
        "id": "LEffdPdYmSjF"
      },
      "id": "LEffdPdYmSjF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We should investigate further which fields that should be dropped\n",
        "df_1[\"Flags\"].unique() # Should we drop it?  ---> YES\n",
        "df_1['Flag Codes'].unique() # Should we drop it?  ---> Yes\n",
        "df_1['Unit Code'].unique() # Should we drop it?\n",
        "df_1['Unit'].unique() # Should we drop it?\n",
        "df_1[\"Indicator\"].unique() # Should we drop it?\n",
        "df_1[\"INDICATOR\"].unique() # Should we drop it?"
      ],
      "metadata": {
        "id": "NPyroT27FaSH"
      },
      "id": "NPyroT27FaSH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's drop in place this time without creating new dataframe\n",
        "df_1.drop(columns=[\"Flags\",\"Flag Codes\"],inplace=True)\n",
        "df_1.head()"
      ],
      "metadata": {
        "id": "--zqKji-G0IC"
      },
      "id": "--zqKji-G0IC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking for null values\n",
        "df_1.isnull().sum()"
      ],
      "metadata": {
        "id": "fhcpAq3_HAh5"
      },
      "id": "fhcpAq3_HAh5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Grouped by country\n",
        "df_1.groupby(\"Country\")['Value'].mean()\n",
        "# df_1.groupby(\"Country\")['Value'].mean().reset_index()"
      ],
      "metadata": {
        "id": "hJmzdMnOzVIl"
      },
      "id": "hJmzdMnOzVIl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The backlash allows us to break up 1 long line of Python code\n",
        "multi_group_var = df_1 \\\n",
        "         .groupby(['Country', 'INDICATOR']) \\\n",
        "         ['Value'] \\\n",
        "         .mean() \\\n",
        "         .reset_index()\n",
        "multi_group_var\n"
      ],
      "metadata": {
        "id": "pO125XcEmSl2"
      },
      "id": "pO125XcEmSl2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many unique inequality for each country\n",
        "df_1.groupby(\"Country\")['Inequality'].nunique()"
      ],
      "metadata": {
        "id": "GUsz8kCn2dgd"
      },
      "id": "GUsz8kCn2dgd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering the data and showing only country = Spain\n",
        "df_1[df_1[\"Country\"]==\"Spain\"]\n"
      ],
      "metadata": {
        "id": "NHHWoQsKmSo0"
      },
      "id": "NHHWoQsKmSo0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Plot\n",
        "#Plotting the first  10 countries with regard to values\n",
        "import matplotlib.pyplot as plt\n",
        "df_1.groupby(\"Country\")['Value'].mean()[:10].plot(kind=\"bar\");\n"
      ],
      "metadata": {
        "id": "S6vt-b8xmSre"
      },
      "id": "S6vt-b8xmSre",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the unit code more further\n",
        "df_1[\"Unit Code\"].unique()"
      ],
      "metadata": {
        "id": "fH7ck-RpHtMl"
      },
      "id": "fH7ck-RpHtMl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying groupby for specific condition\n",
        "# Plot for PC and for HOUR at the same graph\n",
        "\n",
        "subset1=df_1[df_1[\"Unit Code\"]==\"PC\"]\n",
        "subset2=df_1[df_1[\"Unit Code\"]==\"HOUR\"]\n",
        "subset1.groupby(\"Country\")['Value'].mean()[:10].plot(kind=\"bar\",label=\"PC\", color=\"Orange\")\n",
        "subset2.groupby(\"Country\")['Value'].mean()[:10].plot(kind=\"bar\",label=\"HOUR\")\n",
        "plt.legend()\n",
        "\n",
        "# subset.groupby(\"Country\")['Value'].mean()[:10].plot(kind=\"bar\");"
      ],
      "metadata": {
        "id": "sZKO5FB6mSuk"
      },
      "id": "sZKO5FB6mSuk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying groupby for specific condition\n",
        "# Plot for PC and for HOUR at the same graph\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "subset1 = df_1[df_1[\"Unit Code\"] == \"PC\"]\n",
        "subset2 = df_1[df_1[\"Unit Code\"] == \"HOUR\"]\n",
        "\n",
        "# Get the top 10 countries for each subset\n",
        "top_countries1 = subset1.groupby(\"Country\")['Value'].mean().nlargest(10).index\n",
        "top_countries2 = subset2.groupby(\"Country\")['Value'].mean().nlargest(10).index\n",
        "\n",
        "# Combine the top countries from both subsets\n",
        "all_top_countries = list(set(top_countries1) | set(top_countries2))\n",
        "\n",
        "# Filter the subsets to include only the top countries\n",
        "subset1_filtered = subset1[subset1['Country'].isin(all_top_countries)]\n",
        "subset2_filtered = subset2[subset2['Country'].isin(all_top_countries)]\n",
        "\n",
        "# Calculate the mean values for each country in each subset\n",
        "mean_values1 = subset1_filtered.groupby(\"Country\")['Value'].mean()\n",
        "mean_values2 = subset2_filtered.groupby(\"Country\")['Value'].mean()\n",
        "\n",
        "# Plotting the bar chart with adjusted positions and widths\n",
        "width = 0.35  # Width of each bar\n",
        "x = range(len(all_top_countries))\n",
        "\n",
        "plt.bar(x, mean_values1.reindex(all_top_countries, fill_value=0), width, label=\"PC\", color=\"orange\")\n",
        "plt.bar([i + width for i in x], mean_values2.reindex(all_top_countries, fill_value=0), width, label=\"HOUR\")\n",
        "\n",
        "plt.xticks([i + width / 2 for i in x], all_top_countries, rotation=90)\n",
        "plt.ylabel(\"Mean Value\")\n",
        "plt.title(\"Mean Values for PC and HOUR by Country\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ozqrNS0vmSxV"
      },
      "id": "ozqrNS0vmSxV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check the data using Unit Code\n",
        "df_1.groupby(\"Unit Code\")['Indicator'].nunique()"
      ],
      "metadata": {
        "id": "KId9IaISJHjy"
      },
      "id": "KId9IaISJHjy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#It's a good practice to compare countries by its values using same unit. In our case we wanted to keep tracking only the unit\n",
        "#with PC since it has 12 different indicators\n",
        "df_PC=df_1[df_1[\"Unit Code\"]==\"PC\"]\n",
        "df_PC"
      ],
      "metadata": {
        "id": "knI8GSiXH_X0"
      },
      "id": "knI8GSiXH_X0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So unit and unit code columns make no-sense to stay involve in our analysis\n",
        "df_PC.drop(columns=[\"Unit\",\"Unit Code\"],inplace=True)\n",
        "df_PC"
      ],
      "metadata": {
        "id": "bXmnhzI-4QuS"
      },
      "id": "bXmnhzI-4QuS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check other columns: Does other field still contains various data or value?\n",
        "for field in df_PC.columns:\n",
        "  if len(df_PC[field].unique())<=1:\n",
        "    print(field, df_PC[field].unique())\n",
        "#If you did not see anything means you selected a good data context"
      ],
      "metadata": {
        "id": "Km19xff74QxV"
      },
      "id": "Km19xff74QxV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How many indicators for df_PC\n",
        "df_PC[\"Indicator\"].nunique()\n",
        "# list unique indicatros\n",
        "df_PC[\"Indicator\"].unique()"
      ],
      "metadata": {
        "id": "1_mfsOsS4Q0i"
      },
      "id": "1_mfsOsS4Q0i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the value by Indicator in descending order\n",
        "indicator_data= df_PC.groupby(\"Indicator\")['Value'].mean().sort_values(ascending=False)\n",
        "indicator_data"
      ],
      "metadata": {
        "id": "AsoIkfMY4Q3d"
      },
      "id": "AsoIkfMY4Q3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As a starting point, you can use Indicator column to get more insight.\n",
        "#Let's pick water quality and compare its value acros countries\n",
        "water_quality_avg=indicator_data[\"Water quality\"]\n",
        "water_quality_avg"
      ],
      "metadata": {
        "id": "PKGt5jVa7yLW"
      },
      "id": "PKGt5jVa7yLW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wanted to filter only countries that below the average of water quality\n",
        "# Let's pick only the quality water rows first\n",
        "subset_water_quality = df_PC[df_PC[\"Indicator\"]==\"Water quality\"]\n",
        "# Let's group by countries\n",
        "countries_water_quality=subset_water_quality.groupby(\"Country\")['Value'].mean()\n",
        "#Let's filter only the countries that less than the quality water average get them in order descending\n",
        "\n",
        "countries_water_quality[countries_water_quality<water_quality_avg].sort_values(ascending=False)\n"
      ],
      "metadata": {
        "id": "Fy_A5b8X4Q8s"
      },
      "id": "Fy_A5b8X4Q8s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What about plotting all the countries regarding the water qualties average for each\n",
        "#Plotting the following countries_water_quality\n",
        "plt.rcParams['figure.figsize'] = [17, 3]\n",
        "countries_water_quality.plot(kind=\"bar\")\n",
        "#Graph an average line\n",
        "plt.axhline(y=water_quality_avg, color='g', linestyle='-');\n",
        "#The countries with average value less than the average color its bar with red color\n"
      ],
      "metadata": {
        "id": "N0CDcQDbLReA"
      },
      "id": "N0CDcQDbLReA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [17, 3]\n",
        "\n",
        "# Create the bar plot with custom colors\n",
        "bar_colors = np.where(countries_water_quality < water_quality_avg, 'red', 'blue')  # Red for below average, blue otherwise\n",
        "plt.bar(countries_water_quality.index, countries_water_quality.values, color=bar_colors)\n",
        "\n",
        "# Graph an average line\n",
        "plt.axhline(y=water_quality_avg, color='g', linestyle='-')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Country\")\n",
        "plt.ylabel(\"Average Water Quality\")\n",
        "plt.title(\"Water Quality by Country\")\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GAQcTR_QN4lA"
      },
      "id": "GAQcTR_QN4lA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a pivot table for dataframe df_PC using countries as rows and indicator as columns and taking value from value column\n",
        "#If I was oriented to focus more on specific indicators.\n",
        "indicator_focus =[\"Assault rate\", \"Employees working very long hours\", \"Water quality\"]\n",
        "#Create a new data frame with the specfic indicators\n",
        "df_PC_focus=df_PC[df_PC[\"Indicator\"].isin(indicator_focus)]\n",
        "df_PC_focus\n"
      ],
      "metadata": {
        "id": "42ENNfeB-BC7"
      },
      "id": "42ENNfeB-BC7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating a pivote table for df_PC_focus and an average aggregate for value\n",
        "pivot_table = df_PC_focus.pivot_table(index='Country', columns='Indicator', values='Value', aggfunc='mean')\n",
        "pivot_table\n"
      ],
      "metadata": {
        "id": "XMYQIrPYO0ZX"
      },
      "id": "XMYQIrPYO0ZX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a scatter plot using the pivote table to check the relationship between Assault rate and Employees working very long hours\n",
        "plt.rcParams['figure.figsize'] = [17, 10]\n",
        "pivot_table.plot(kind=\"scatter\",x=\"Assault rate\",y=\"Employees working very long hours\")\n"
      ],
      "metadata": {
        "id": "boeMVHm9-BFt"
      },
      "id": "boeMVHm9-BFt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find more ways to check the relationship"
      ],
      "metadata": {
        "id": "shjdudCSQC0K"
      },
      "id": "shjdudCSQC0K",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}